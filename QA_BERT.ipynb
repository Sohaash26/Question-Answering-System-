{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul80E8mxAHPz"
      },
      "source": [
        "# Questions Answering System using BERT:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71KpoYIoAHP2"
      },
      "source": [
        "### Importing Libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "v5Jlcrg0ghs-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "import nltk\n",
        "import spacy\n",
        "import string\n",
        "#import evaluate  # Bleu\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import transformers\n",
        "from transformers import AutoTokenizer,AdamW,BertForQuestionAnswering\n",
        "from transformers import AutoTokenizer, DistilBertForQuestionAnswering, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import time\n",
        "import os\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "uC-Kx4ij3T6a"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X55oFpqzd78h"
      },
      "source": [
        "## Loading and Reading SQuAD 2.0 dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "U7XcbS3afYOf"
      },
      "outputs": [],
      "source": [
        " %%capture\n",
        "!mkdir squad\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json -O squad/train-v2.0.json\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json -O squad/dev-v2.0.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "6b2EMVb-fhwA"
      },
      "outputs": [],
      "source": [
        "# Train file path.\n",
        "path = Path('squad/train-v2.0.json')\n",
        "\n",
        "with open(path, 'rb') as f:\n",
        "    squad_dict = json.load(f)\n",
        "\n",
        "texts = []\n",
        "queries = []\n",
        "answers = []\n",
        "\n",
        "# Searching for each passage, its question and answer.\n",
        "for group in squad_dict['data']:\n",
        "    for passage in group['paragraphs']:\n",
        "        context = passage['context']\n",
        "        for qa in passage['qas']:\n",
        "            question = qa['question']\n",
        "            for answer in qa['answers']:\n",
        "                # Storing all passage, and its answer to the list.\n",
        "                texts.append(context)\n",
        "                queries.append(question)\n",
        "                answers.append(answer)\n",
        "\n",
        "train_texts, train_queries, train_answers = texts, queries, answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "_JPP5Daci2V8"
      },
      "outputs": [],
      "source": [
        "# Validation file path.\n",
        "path = Path('squad/dev-v2.0.json')\n",
        "\n",
        "with open(path, 'rb') as f:\n",
        "    squad_dict = json.load(f)\n",
        "\n",
        "texts = []\n",
        "queries = []\n",
        "answers = []\n",
        "\n",
        "# Searching for each passage, its question and answer.\n",
        "for group in squad_dict['data']:\n",
        "    for passage in group['paragraphs']:\n",
        "        context = passage['context']\n",
        "        for qa in passage['qas']:\n",
        "            question = qa['question']\n",
        "            for answer in qa['answers']:\n",
        "                # Storing all passage, and its answer to the list.\n",
        "                texts.append(context)\n",
        "                queries.append(question)\n",
        "                answers.append(answer)\n",
        "\n",
        "val_texts, val_queries, val_answers = texts, queries, answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEN89d8Mjlw3"
      },
      "source": [
        "## Checking the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3f9hbvMjJ1b",
        "outputId": "2a62cdd4-e7de-44db-a8c8-67fa43d8887d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86821\n",
            "86821\n",
            "86821\n"
          ]
        }
      ],
      "source": [
        "print(len(train_texts))\n",
        "print(len(train_queries))\n",
        "print(len(train_answers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPrlBV3hkJSe",
        "outputId": "1720b670-1a58-437d-a67b-7d494557d0ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passage:  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
            "Query:  When did Beyonce start becoming popular?\n",
            "Answer:  {'text': 'in the late 1990s', 'answer_start': 269}\n"
          ]
        }
      ],
      "source": [
        "print(\"Passage: \",train_texts[0])\n",
        "print(\"Query: \",train_queries[0])\n",
        "print(\"Answer: \",train_answers[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMs6opQPlLBE",
        "outputId": "b8c0d56a-5ed2-4b42-ed6b-3d17180bb3df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20302\n",
            "20302\n",
            "20302\n"
          ]
        }
      ],
      "source": [
        "print(len(val_texts))\n",
        "print(len(val_queries))\n",
        "print(len(val_answers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBf0eNdblL_P",
        "outputId": "a4f8bb06-024c-447e-aced-a5fec158699b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passage:  The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.\n",
            "Query:  In what country is Normandy located?\n",
            "Answer:  {'text': 'France', 'answer_start': 159}\n"
          ]
        }
      ],
      "source": [
        "print(\"Passage: \",val_texts[0])\n",
        "print(\"Query: \",val_queries[0])\n",
        "print(\"Answer: \",val_answers[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79TEci2gnAP4"
      },
      "source": [
        "## Find the start and end position character"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "qlexBsEcnI77"
      },
      "outputs": [],
      "source": [
        "for answer, text in zip(train_answers, train_texts):\n",
        "    real_answer = answer['text']\n",
        "    start_idx = answer['answer_start']\n",
        "    end_idx = start_idx + len(real_answer)\n",
        "\n",
        "    if text[start_idx:end_idx] == real_answer:\n",
        "        answer['answer_end'] = end_idx\n",
        "    elif text[start_idx-1:end_idx-1] == real_answer:\n",
        "        answer['answer_start'] = start_idx - 1\n",
        "        answer['answer_end'] = end_idx - 1\n",
        "    elif text[start_idx-2:end_idx-2] == real_answer:\n",
        "        answer['answer_start'] = start_idx - 2\n",
        "        answer['answer_end'] = end_idx - 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Xe2_AZszsRqr"
      },
      "outputs": [],
      "source": [
        "for answer, text in zip(val_answers, val_texts):\n",
        "    real_answer = answer['text']\n",
        "    start_idx = answer['answer_start']\n",
        "    end_idx = start_idx + len(real_answer)\n",
        "\n",
        "    if text[start_idx:end_idx] == real_answer:\n",
        "        answer['answer_end'] = end_idx\n",
        "    elif text[start_idx-1:end_idx-1] == real_answer:\n",
        "        answer['answer_start'] = start_idx - 1\n",
        "        answer['answer_end'] = end_idx - 1\n",
        "    elif text[start_idx-2:end_idx-2] == real_answer:\n",
        "        answer['answer_start'] = start_idx - 2\n",
        "        answer['answer_end'] = end_idx - 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83nKp3c5tU7A"
      },
      "source": [
        "## Tokenization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dncUoMhtpWj"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "train_encodings = tokenizer(train_texts, train_queries, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_texts, val_queries, truncation=True, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArPLAzqlAHP7"
      },
      "outputs": [],
      "source": [
        "# OPTIMIZER = AdamW(model.parameters(), lr=0.00001)\n",
        "# Q_LEN = 256   # Question Length\n",
        "# T_LEN = 32    # Target Length\n",
        "# BATCH_SIZE = 4\n",
        "# DEVICE = \"cuda:0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHaETr5izn0t"
      },
      "source": [
        "### Convert the start-end positions to tokens start-end positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHDTX4HZzN3e"
      },
      "outputs": [],
      "source": [
        "def add_token_positions(encodings, answers):\n",
        "  start_positions = []\n",
        "  end_positions = []\n",
        "\n",
        "  count = 0\n",
        "\n",
        "  for i in range(len(answers)):\n",
        "    start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
        "    end_positions.append(encodings.char_to_token(i, answers[i]['answer_end']))\n",
        "\n",
        "    if start_positions[-1] is None:\n",
        "      start_positions[-1] = tokenizer.model_max_length\n",
        "\n",
        "    if end_positions[-1] is None:\n",
        "      end_positions[-1] = encodings.char_to_token(i, answers[i]['answer_end'] - 1)\n",
        "\n",
        "      if end_positions[-1] is None:\n",
        "        count += 1\n",
        "        end_positions[-1] = tokenizer.model_max_length\n",
        "\n",
        "  print(count)\n",
        "\n",
        "  encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
        "\n",
        "add_token_positions(train_encodings, train_answers)\n",
        "add_token_positions(val_encodings, val_answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZTuiwEwAHP8"
      },
      "outputs": [],
      "source": [
        "class QA_Dataset(Dataset):\n",
        "    def __init__(self, tokenizer, dataframe, q_len, t_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.q_len = q_len\n",
        "        self.t_len = t_len\n",
        "        self.data = dataframe\n",
        "        self.questions = self.data[\"question\"]\n",
        "        self.context = self.data[\"context\"]\n",
        "        self.answer = self.data['answer']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question = self.questions[idx]\n",
        "        context = self.context[idx]\n",
        "        answer = self.answer[idx]\n",
        "\n",
        "        question_tokenized = self.tokenizer(question, context, max_length=self.q_len, padding=\"max_length\",\n",
        "                                                    truncation=True, pad_to_max_length=True, add_special_tokens=True)\n",
        "        answer_tokenized = self.tokenizer(answer, max_length=self.t_len, padding=\"max_length\",\n",
        "                                          truncation=True, pad_to_max_length=True, add_special_tokens=True)\n",
        "\n",
        "        labels = torch.tensor(answer_tokenized[\"input_ids\"], dtype=torch.long)\n",
        "        labels[labels == 0] = -100\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(question_tokenized[\"input_ids\"], dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor(question_tokenized[\"attention_mask\"], dtype=torch.long),\n",
        "            \"labels\": labels,\n",
        "            \"decoder_attention_mask\": torch.tensor(answer_tokenized[\"attention_mask\"], dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2qNeiMsAHP8"
      },
      "outputs": [],
      "source": [
        "# # Dataloader\n",
        "\n",
        "# train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# train_sampler = RandomSampler(train_data.index)\n",
        "# val_sampler = RandomSampler(val_data.index)\n",
        "\n",
        "# qa_dataset = QA_Dataset(tokenizer, data, Q_LEN, T_LEN)\n",
        "\n",
        "# train_loader = DataLoader(qa_dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
        "# val_loader = DataLoader(qa_dataset, batch_size=BATCH_SIZE, sampler=val_sampler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JNdLyE97io2"
      },
      "source": [
        "## Create a Dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjYzrXas3yhy"
      },
      "outputs": [],
      "source": [
        "class SquadDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHGYpU8x8GCT"
      },
      "outputs": [],
      "source": [
        "train_dataset = SquadDataset(train_encodings)\n",
        "val_dataset = SquadDataset(val_encodings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCohXB_R8HkC"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N12rPAE39QS0"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available()\n",
        "                      else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sA00XY9LAHP9"
      },
      "outputs": [],
      "source": [
        "# MODEL = MODEL.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctnv1lVt-Na_"
      },
      "source": [
        "## Bert Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unmSIjkW-ZEH"
      },
      "outputs": [],
      "source": [
        "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased').to(device)\n",
        "# OPTIMIZER = Adam(MODEL.parameters(), lr=0.00001)\n",
        "# DEVICE = \"cuda:0\"\n",
        "optim = AdamW(model.parameters(), lr=5e-5)\n",
        "Q_LEN = 256   # Question Length\n",
        "T_LEN = 32    # Target Length\n",
        "BATCH_SIZE = 4\n",
        "epochs = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZW7oZ9kAHP9"
      },
      "outputs": [],
      "source": [
        "#with open(r'./squad/dev-v2.0.json') as f:\n",
        "  #  data = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48-TcF-nAHP9"
      },
      "outputs": [],
      "source": [
        "#with open(r'./squad/dev-v2.0.json') as f:\n",
        "      # try:\n",
        "      #     data = json.load(f)\n",
        "      # except JSONDecodeError as e:\n",
        "       #    print(f\"Error loading JSON data: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6eFwOKYAHP-",
        "outputId": "e037a9af-2d01-45b5-f136-6cc7a54ddd0e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
              "      <td>In what country is Normandy located?</td>\n",
              "      <td>France</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
              "      <td>When were the Normans in Normandy?</td>\n",
              "      <td>10th and 11th centuries</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
              "      <td>From which countries did the Norse originate?</td>\n",
              "      <td>Denmark, Iceland and Norway</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
              "      <td>Who was the Norse leader?</td>\n",
              "      <td>Rollo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
              "      <td>What century did the Normans first gain their ...</td>\n",
              "      <td>10th century</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             context  \\\n",
              "0  The Normans (Norman: Nourmands; French: Norman...   \n",
              "1  The Normans (Norman: Nourmands; French: Norman...   \n",
              "2  The Normans (Norman: Nourmands; French: Norman...   \n",
              "3  The Normans (Norman: Nourmands; French: Norman...   \n",
              "4  The Normans (Norman: Nourmands; French: Norman...   \n",
              "\n",
              "                                            question  \\\n",
              "0               In what country is Normandy located?   \n",
              "1                 When were the Normans in Normandy?   \n",
              "2      From which countries did the Norse originate?   \n",
              "3                          Who was the Norse leader?   \n",
              "4  What century did the Normans first gain their ...   \n",
              "\n",
              "                        answer  \n",
              "0                       France  \n",
              "1      10th and 11th centuries  \n",
              "2  Denmark, Iceland and Norway  \n",
              "3                        Rollo  \n",
              "4                 10th century  "
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XO8Ob7XLAHP-"
      },
      "outputs": [],
      "source": [
        "# Dataloader\n",
        "\n",
        "#train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "#train_sampler = RandomSampler(train_data.index)\n",
        "#val_sampler = RandomSampler(val_data.index)\n",
        "\n",
        "#qa_dataset = QA_Dataset(tokenizer, data, Q_LEN, T_LEN)\n",
        "\n",
        "#train_loader = DataLoader(qa_dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
        "#val_loader = DataLoader(qa_dataset, batch_size=BATCH_SIZE, sampler=val_sampler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MWoZePiAHP-"
      },
      "outputs": [],
      "source": [
        "# train_loss = 0\n",
        "# val_loss = 0\n",
        "# train_batch_count = 0\n",
        "# val_batch_count = 0\n",
        "\n",
        "# for epoch in range(2):\n",
        "#     model.train()\n",
        "#     for batch in tqdm(train_loader, desc=\"Training batches\"):\n",
        "#         input_ids = batch[\"input_ids\"].to(device)\n",
        "#         attention_mask = batch[\"attention_mask\"].to(device)\n",
        "#         labels = batch[\"labels\"].to(device)\n",
        "#         decoder_attention_mask = batch[\"decoder_attention_mask\"].to(device)\n",
        "\n",
        "#         outputs = model(\n",
        "#                           input_ids=input_ids,\n",
        "#                           attention_mask=attention_mask,\n",
        "#                           labels=labels,\n",
        "#                           decoder_attention_mask=decoder_attention_mask\n",
        "#                         )\n",
        "\n",
        "#         optim.zero_grad()\n",
        "#         outputs.loss.backward()\n",
        "#         optim.step()\n",
        "#         train_loss += outputs.loss.item()\n",
        "#         train_batch_count += 1\n",
        "\n",
        "#     #Evaluation\n",
        "#     model.eval()\n",
        "#     for batch in tqdm(val_loader, desc=\"Validation batches\"):\n",
        "#         input_ids = batch[\"input_ids\"].to(device)\n",
        "#         attention_mask = batch[\"attention_mask\"].to(device)\n",
        "#         labels = batch[\"labels\"].to(device)\n",
        "#         decoder_attention_mask = batch[\"decoder_attention_mask\"].to(device)\n",
        "\n",
        "#         outputs = model(\n",
        "#                           input_ids=input_ids,\n",
        "#                           attention_mask=attention_mask,\n",
        "#                           labels=labels,\n",
        "#                           decoder_attention_mask=decoder_attention_mask\n",
        "#                         )\n",
        "\n",
        "#         optim.zero_grad()\n",
        "#         outputs.loss.backward()\n",
        "#         optim.step()\n",
        "#         val_loss += outputs.loss.item()\n",
        "#         val_batch_count += 1\n",
        "\n",
        "#     print(f\"{epoch+1}/{2} -> Train loss: {train_loss / train_batch_count}\\tValidation loss: {val_loss/val_batch_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9d517VxAHQN"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(r\"\\Group13_CW2/qa_model\")\n",
        "tokenizer.save_pretrained(r\"\\Group13_CW2/qa_tokenizer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe29xEPB_DvT"
      },
      "source": [
        "## Training and Evaluating the Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qkwmHdB_DKI"
      },
      "outputs": [],
      "source": [
        "whole_train_eval_time = time.time()\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "print_every = 50\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  epoch_time = time.time()\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  loss_of_epoch = 0\n",
        "\n",
        "  print(\"Train:\")\n",
        "\n",
        "  for batch_idx,batch in enumerate(train_loader):\n",
        "\n",
        "    optim.zero_grad()\n",
        "\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    start_positions = batch['start_positions'].to(device)\n",
        "    end_positions = batch['end_positions'].to(device)\n",
        "\n",
        "    outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "    loss = outputs[0]\n",
        "    loss.backward()\n",
        "\n",
        "    optim.step()\n",
        "\n",
        "    loss_of_epoch += loss.item()\n",
        "\n",
        "    if (batch_idx+1) % print_every == 0:\n",
        "      print(\"Batch {:} / {:}\".format(batch_idx+1,len(train_loader)),\"\\nLoss:\", round(loss.item(),1),\"\\n\")\n",
        "\n",
        "  loss_of_epoch /= len(train_loader)\n",
        "  train_losses.append(loss_of_epoch)\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  print(\"Evaluate:\")\n",
        "\n",
        "  loss_of_epoch = 0\n",
        "\n",
        "  for batch_idx,batch in enumerate(val_loader):\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      start_positions = batch['start_positions'].to(device)\n",
        "      end_positions = batch['end_positions'].to(device)\n",
        "\n",
        "      outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "      loss = outputs[0]\n",
        "      loss_of_epoch += loss.item()\n",
        "\n",
        "    if (batch_idx+1) % print_every == 0:\n",
        "       print(\"Batch {:} / {:}\".format(batch_idx+1,len(val_loader)),\"\\nLoss:\", round(loss.item(),1),\"\\n\")\n",
        "\n",
        "  loss_of_epoch /= len(val_loader)\n",
        "  val_losses.append(loss_of_epoch)\n",
        "\n",
        "  print(\"\\n-------Epoch \", epoch+1,\n",
        "        \"-------\"\n",
        "        \"\\nTraining Loss:\", train_losses[-1],\n",
        "        \"\\nValidation Loss:\", val_losses[-1],\n",
        "        \"\\nTime: \",(time.time() - epoch_time),\n",
        "        \"\\n-----------------------\",\n",
        "        \"\\n\\n\")\n",
        "\n",
        "print(\"Total training and evaluation time: \", (time.time() - whole_train_eval_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Daxw8aQMHPTV"
      },
      "outputs": [],
      "source": [
        "fig,ax = plt.subplots(1,1,figsize=(15,10))\n",
        "\n",
        "ax.set_title(\"Train and Valid Losses\",size=15)\n",
        "ax.set_ylabel('Loss', fontsize = 20)\n",
        "ax.set_xlabel('Epochs', fontsize = 20)\n",
        "_=ax.plot(train_losses)\n",
        "_=ax.plot(val_losses)\n",
        "_=ax.legend(('Train','Val'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75KNQmRfAHQO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}